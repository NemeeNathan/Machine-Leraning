1.sublist
a=[5,6,3,8,2,1,7,1]
print("The original list:"+str(a))
b=[8,2,1]
res=False
for idx in range (len(a)-len(b)+1):
    if a[idx:idx+len(b)]==b:
        res=True
        break
print("Is sublist present in list?:"+str(res))
------------------------------------------------------
2.Average
def average(a):
    for d in a:
        n1=d.pop('rank')
        n2=d.pop('rank2')
        d['rank+rank2']=(n1+n2)/2
    return a
data=[
    {'id':1,'Role':'Data Maintaining','rank':70,'rank2':82},
    {'id':2,'Role':'web develop','rank':73,'rank2':74}
]
print(average(data))
  #  for i in average(data):
   # print(i)
-------------------------------------------------------------
3.Slice
L=tuple("HELLO WORLD")
L2=(1,5,2,7,9,0,8,9,6,4)
print(type(L2))
print(L)
print(L[::-1])
print(L[0:])
print(L[:5])
print(L2[::-1])
print(L2[0:])
print(L2[:5])
-------------------------------------------------------------
4.Numpy
import numpy as np
mat=np.array([[1,2,3],[1,2,3]])
mat1=np.array([[1,2,3],[1,2,3]])
print(np.multiply(mat,mat1))
print(np.divide(mat,mat1))
-------------------------------------------------------------
5.Power of Array(Pandas)
import pandas as pd
m={"values":[1,2,3,4,5,6,76,78,89,90]}
df=pd.DataFrame(m)
df['sq']=df["values"]**2
print(df)
-------------------------------------------------------------
6.Pangram
text=input().lower()
print(text)
l=list(set(text))
if " " in l:
    l.remove(" ")
if len(l)==26:
    print("anangaram")
else:
    print("not an anagram")
print(l)
#The quick brown fox jumps over the lazy dog
-------------------------------------------------------------
7.K-fold
from numpy import array
from sklearn.model_selection import KFold

data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])
kfold = KFold(6)

for train, test in kfold.split(data):
    print('train:%s, test: %s' % (data[train], data[test]))
-------------------------------------------------------------------
8.CSV file list of string
import csv
with open('demo.csv') as file_obj:
    reader_obj = csv.reader(file_obj)
    for row in reader_obj:
        print(row)
department_id, department_name,  manager_id,  location_id
10, Administration, 200, 1700
20, Marketing, 201, 1800
30, Purchasing, 114, 1700
40, Human Resources, 203, 2400
50, Shipping, 121, 1500
60, IT, 103, 1400
70, Public Relations, 204, 2700
80, Sales, 145, 2500
---------------------------------------------------------------------
9.K-Means
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

import sys
x = [4,5,10,4,3,11,14,6,10,12]
y = [21,19,24,17,16,25,24,22,21,21]

plt.scatter(x, y)
plt.show()

data = list(zip(x, y))
kmeans = KMeans(n_clusters=2)
kmeans.fit(data)

plt.scatter(x, y, c=kmeans.labels_)
plt.show()
plt.savefig(sys.stdout.buffer)
sys.stdout.flush()
---------------------------------------------------------------------
10.Naive Bayes
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

df = pd.read_csv("demo.csv")
x = df.drop("diabetes",axis=1)
y = df["diabetes"]
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)
model = GaussianNB()
model.fit(x_train,y_train)
y_pred=model.predict(x_test)
accuracy=accuracy_score(y_test,y_pred)*100
print(accuracy)
#https://github.com/dhirajk100/Naive-Bayes
---------------------------------------------------------------------
11.Logistic Regression

import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LogisticRegression
# from sklearn.datasets import load_iris

if __name__ == "__main__":
    data = pd.read_csv('irisdata.csv')

    x = data[['length', 'width']]
    y = data['type']
    model = LogisticRegression(solver='liblinear', random_state=0)
    model.fit(x,y)
    z=model.predict(x)
    print(z)
    plt.plot(z)
    plt.show()
#dataset
---------------------------------------------------------------------
12.stack Generation
import sklearn.datasets as sd
import sklearn.model_selection as al
import sklearn.ensemble as se
import sklearn.linear_model as sl
import sklearn.svm as sv
import time
X, Y = sd.load_diabetes(return_X_y=True)
X_train, X_test, Y_train, Y_test = al.train_test_split(X,Y,random_state=42)
stacked = se.StackingRegressor( estimators =[('SVR', sv.SVR()),('Liner',sl.LinearRegression())])
st = time.time()
stacked.fit(X_train, Y_train)
et = time.time()
print("Coefficient of determination: {}".format(stacked.score(X_test, Y_test)))
print("Computation Time: {}".format(et - st))
---------------------------------------------------------------------------
13.svm
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn import svm


data = pd.read_csv('#irisdata.csv')
x = data['length']
y = data['width']
tr_x = np.vstack((x, y)).T
tr_y = data['Type']
clf = svm.SVC(kernel='linear', C=1.0)
clf.fit(tr_x,tr_y)
w =clf.coef_[0]
a = -w[0] /w[1]
xx = np.linspace(0, 13)
yy = a* xx -clf.intercept_[0] / w[1]
plt.plot(xx,yy,'k-')
plt.scatter(tr_x[:,0],tr_x[:,1],c=tr_y)
plt.show()
------------------------------------------------------------------------
Dataset:10.Naive-Bayes
glucose,bloodpressure,diabetes
40,85,0
40,92,0
45,63,1
45,80,0
40,73,1
45,82,0
40,85,0
30,63,1
65,65,1
45,82,0
35,73,1
45,90,0
50,68,1
40,93,0
35,80,1
50,70,1
40,73,1
40,67,1
40,75,1
40,80,1
40,72,1
40,88,0
40,78,1
45,98,0
40,88,0
60,67,1
40,85,0
40,88,0
45,78,0
55,73,1
45,77,1
50,68,1
45,77,1
40,85,0
45,70,1
45,72,1
45,90,0
40,65,1
45,88,0
45,88,0
40,68,1
40,73,1
45,88,0
45,78,0
45,85,0
40,83,0
40,63,1
45,73,1
45,90,0
45,87,0
40,90,0
45,93,0
50,73,1
40,68,1
50,68,1
50,90,0
50,75,1
50,85,0
45,83,0
50,65,1
45,80,0
40,75,1
35,77,1
55,68,1
45,85,0
45,87,0
25,82,1
40,90,0
45,82,0
45,80,0
45,88,0
40,87,0
45,70,1
45,88,0
45,88,0
50,67,1
45,93,0
45,90,0
55,77,1
35,62,1
30,78,1
50,82,0
45,83,0
40,83,0
40,82,0
50,70,1
45,92,0
40,90,0
40,85,0
40,73,1
50,88,0
30,80,1
40,85,1
30,75,1
45,85,0
40,85,0
60,80,1
40,90,0
35,65,1
40,87,0
45,87,0
45,92,0
45,95,0
50,67,1
40,90,0
45,85,0
45,75,1
50,73,1
40,70,1
40,92,0
30,72,1
50,87,0
40,93,0
55,60,1
40,82,1
40,95,0
40,72,1
60,68,1
45,92,0
40,70,1
45,97,0
50,83,0
60,63,1
35,65,1
45,90,0
40,65,1
40,95,0
30,68,1
45,83,0
45,92,0
45,87,0
45,72,1
45,83,0
45,85,0
45,88,0
55,68,1
60,65,1
40,73,1
35,73,1
40,83,0
50,67,1
45,80,0
55,68,1
45,80,0
50,80,0
40,92,0
50,77,1
50,92,0
45,85,0
65,70,1
45,77,1
45,82,0
40,95,0
45,75,1
45,78,1
45,87,0
50,83,0
45,92,0
40,73,1
45,88,0
45,72,1
40,67,1
45,78,0
40,78,0
45,87,0
50,78,1
45,75,1
55,73,1
45,87,0
45,80,0
45,73,1
45,93,0
45,73,1
40,87,0
40,87,0
45,87,0
25,72,1
45,88,0
55,68,1
40,90,0
40,93,0
45,82,0
35,77,1
50,72,1
40,100,0
25,83,1
55,72,1
45,82,0
40,75,1
35,80,1
40,90,0
45,90,0
45,85,0
55,63,1
45,92,0
40,87,0
45,93,0
45,85,0
35,70,1
55,73,1
50,67,1
50,65,1
55,75,1
45,85,0
35,68,1
40,80,0
40,63,1
40,90,0
50,90,0
25,67,1
55,67,1
60,67,1
50,92,0
45,80,0
50,77,1
40,88,0
45,93,0
40,93,0
45,77,0
40,77,0
55,75,1
45,87,0
60,67,1
45,90,0
30,73,1
45,87,0
40,88,0
45,95,0
45,77,1
55,68,1
45,83,0
40,90,0
40,83,0
45,97,0
45,85,0
45,83,0
45,72,1
45,68,1
40,93,0
40,87,0
40,87,0
55,70,1
60,68,1
50,90,0
40,90,0
40,78,1
50,80,1
55,75,1
40,72,1
50,73,1
45,58,1
55,68,1
40,90,0
55,72,1
35,82,0
40,70,1
55,57,1
50,80,0
45,83,0
45,85,0
45,72,1
40,75,1
40,85,0
40,83,0
40,72,1
35,63,1
20,70,1
40,92,0
45,87,0
45,83,0
55,67,1
45,80,0
45,75,1
40,70,1
40,88,0
35,78,1
55,63,1
40,82,0
40,65,1
45,90,0
40,72,1
55,62,1
50,83,0
50,58,1
45,72,1
50,68,1
65,60,1
25,73,1
35,68,1
45,58,1
45,92,0
45,67,1
50,72,1
40,87,0
35,77,1
50,65,1
60,77,1
40,68,1
45,88,1
50,77,1
45,82,0
50,73,1
35,68,1
40,92,0
55,65,1
45,83,0
50,67,1
40,68,1
45,83,0
45,90,0
45,83,0
40,72,1
45,78,1
55,68,1
35,82,1
50,87,0
50,83,0
45,73,0
45,83,0
30,73,1
45,83,0
40,68,1
35,77,1
45,85,0
45,78,1
25,73,1
40,88,0
45,82,1
60,68,1
70,65,1
40,87,0
35,70,1
55,68,1
35,90,0
40,65,1
40,65,1
55,60,1
50,83,0
40,87,0
40,82,0
45,85,0
40,85,0
55,68,1
40,83,0
50,88,0
40,88,0
50,85,0
35,62,1
40,75,1
40,75,1
45,90,0
60,85,1
50,85,0
40,82,0
40,63,1
40,88,0
30,82,1
45,83,0
50,77,1
45,97,0
45,93,0
50,68,1
40,87,0
45,87,0
40,67,1
50,85,0
50,90,0
35,70,1
45,92,0
30,78,1
45,88,0
55,70,1
45,88,0
50,78,1
40,70,1
45,73,1
40,88,0
35,75,1
45,82,0
50,68,1
35,77,1
40,73,1
45,75,1
45,82,0
45,78,1
40,70,1
45,88,0
35,77,1
50,65,1
40,90,0
45,83,0
50,67,1
45,78,1
45,82,0
45,85,0
40,70,1
45,68,1
45,73,1
40,82,0
50,78,1
50,92,0
45,82,0
45,92,0
55,65,1
40,72,1
50,85,0
50,62,1
45,92,0
55,72,1
40,83,0
45,67,1
55,65,1
45,73,1
50,85,0
45,90,0
40,72,1
50,92,0
45,87,0
45,75,1
45,78,0
55,73,1
35,90,0
40,70,1
40,88,0
45,95,0
40,77,1
25,88,1
40,88,0
65,62,1
40,85,0
30,83,1
50,52,1
50,75,1
35,78,1
45,87,0
40,93,0
45,82,0
45,67,1
55,70,1
40,82,0
40,90,0
45,67,1
40,80,1
40,60,1
40,83,0
40,88,0
50,90,0
50,83,0
50,68,1
45,82,0
55,70,1
35,72,1
50,87,0
45,90,0
45,90,0
45,92,0
45,68,1
45,90,0
50,88,0
45,92,0
45,88,0
45,80,0
55,72,1
35,83,0
50,85,0
50,70,1
40,83,0
40,92,0
50,88,0
40,100,0
40,77,1
35,70,1
35,85,1
45,88,0
40,73,1
40,65,1
40,97,0
35,87,1
40,83,0
50,75,1
45,78,1
50,95,0
50,90,0
40,78,1
30,75,1
45,67,1
50,83,0
45,80,0
45,85,0
60,68,1
55,67,1
30,82,1
45,92,0
45,62,1
40,88,0
35,78,1
40,75,1
30,70,1
30,78,1
30,78,1
45,85,1
50,60,1
40,92,0
45,73,1
40,78,0
50,72,1
45,73,1
40,88,0
45,90,0
40,83,0
45,73,1
45,68,1
55,65,1
45,85,0
50,63,1
40,70,1
50,65,1
50,75,1
40,88,0
45,77,1
40,93,0
45,87,0
45,77,0
40,87,0
35,73,1
40,75,1
45,87,0
30,77,1
40,72,1
45,77,1
40,93,0
35,68,1
40,75,1
25,70,1
40,85,0
50,77,1
45,88,0
45,78,1
50,68,1
40,65,1
50,78,0
40,60,1
40,82,0
40,82,1
50,80,1
50,83,0
35,87,1
40,92,0
45,88,0
55,68,1
50,80,0
50,72,1
65,72,1
40,85,0
50,63,1
45,92,0
30,78,1
50,88,0
40,85,0
50,90,0
45,73,1
50,60,1
45,85,0
55,70,1
35,70,1
50,80,0
45,87,0
45,65,1
45,70,1
45,85,0
40,63,1
40,87,1
45,83,0
50,87,0
45,82,1
50,90,0
50,80,0
35,88,0
40,87,0
45,83,0
45,80,0
40,83,1
45,87,0
40,95,0
40,88,0
45,88,0
45,83,0
45,78,1
45,57,1
50,83,0
45,82,0
45,57,1
45,83,0
45,77,1
30,83,1
50,75,0
40,87,0
35,62,1
45,78,0
55,75,1
45,88,0
45,68,1
30,73,1
45,73,1
50,73,1
40,68,1
35,80,1
40,85,0
45,73,1
40,92,0
35,77,1
40,93,0
55,78,1
45,73,1
55,67,1
45,68,1
45,93,0
40,92,0
50,92,0
65,63,1
20,80,1
45,70,1
50,88,0
45,72,1
65,73,1
50,85,0
30,80,1
45,63,1
45,87,0
40,83,0
45,80,0
55,75,1
50,78,0
45,88,0
45,77,1
45,90,0
50,88,0
40,87,0
50,83,0
45,83,0
40,92,0
45,67,1
50,85,1
70,62,1
40,87,0
45,88,0
35,77,1
45,85,0
45,90,0
45,88,0
55,60,1
45,70,1
45,75,1
45,92,0
50,82,0
50,80,1
40,67,1
45,93,0
45,92,0
40,87,0
45,87,1
50,85,0
55,73,1
45,73,1
35,73,1
45,88,0
50,90,0
30,80,1
25,73,1
35,80,1
40,65,1
45,73,0
35,75,1
45,70,1
60,70,1
55,75,1
45,68,1
35,83,1
45,67,1
45,85,0
55,63,1
45,88,1
45,57,1
50,58,1
50,75,1
45,92,0
40,85,0
45,80,0
40,88,0
50,83,0
45,90,0
45,82,1
40,82,1
45,75,1
50,93,0
45,75,1
45,93,0
50,72,1
50,73,1
40,82,0
40,90,0
35,67,1
40,93,0
50,70,1
50,85,0
40,90,0
40,80,1
40,87,0
50,85,0
40,88,0
40,82,1
50,85,0
40,85,1
45,68,1
60,60,1
50,83,0
50,67,1
50,78,0
40,70,1
40,70,1
45,77,0
45,88,0
45,87,0
50,62,1
50,63,1
40,72,1
40,90,0
65,73,1
55,73,1
40,67,1
45,80,0
45,90,0
45,82,0
40,87,0
45,88,0
40,67,1
45,85,0
50,88,0
60,75,1
45,60,1
35,72,1
50,77,0
40,75,1
55,73,1
40,63,1
45,90,0
45,92,0
40,98,0
40,83,0
60,67,1
45,88,0
40,72,1
45,82,1
40,82,1
45,87,0
45,88,0
55,67,1
40,67,1
45,85,0
60,75,1
40,80,1
45,68,1
40,93,0
45,83,0
45,70,1
25,73,1
55,93,0
55,67,1
55,62,1
60,68,1
50,78,1
55,78,1
45,88,0
50,85,0
35,83,1
45,83,0
45,75,1
50,70,1
45,85,0
50,87,0
45,78,1
40,93,0
30,78,1
50,70,1
35,90,0
45,83,0
60,62,1
45,92,0
40,62,1
50,75,1
40,65,1
50,90,0
30,75,1
35,67,1
40,70,1
40,78,0
50,93,0
45,87,0
45,90,0
45,85,0
40,77,1
50,95,0
45,90,0
35,80,1
45,83,0
45,90,0
45,95,0
35,73,1
60,70,1
45,92,0
45,82,0
45,70,1
45,77,1
30,70,1
40,85,0
45,67,1
55,68,1
45,80,1
55,72,1
35,67,1
50,78,1
35,82,0
50,77,1
45,92,0
45,85,0
45,75,1
50,88,0
40,87,0
40,73,1
45,63,1
50,67,1
55,73,1
35,82,0
45,85,0
45,85,0
40,65,1
40,85,0
45,80,0
40,87,0
55,77,1
40,67,1
45,82,0
50,78,1
50,83,0
50,65,1
40,87,0
45,93,0
50,88,0
45,85,0
45,92,0
45,68,1
55,72,1
40,77,1
50,65,1
40,75,1
40,80,0
40,92,0
40,75,1
45,83,1
45,87,0
35,78,1
50,85,0
50,65,1
40,88,0
45,73,1
45,87,0
40,87,0
50,92,0
40,87,0
50,85,0
45,70,1
35,83,0
40,88,0
20,73,1
45,60,1
45,88,0
55,77,1
40,87,0
40,87,0
45,82,0
50,80,1
50,95,0
40,67,1
45,67,1
45,85,0
45,78,0
40,88,0
35,72,1
45,80,0
45,85,0
45,88,0
40,87,0
35,70,1
50,82,0
45,87,0
45,80,0
40,78,0
45,80,0
45,72,1
45,77,1
40,88,0
45,87,0
45,90,0
45,83,0
45,88,0
35,88,0
60,63,1
40,80,1
45,87,0
45,90,0
35,82,0
40,72,1
45,72,1
40,90,0
55,87,0
30,77,1
45,85,0
45,72,1
45,87,0
40,87,0
40,70,1
50,83,0
40,80,0
45,83,0
50,82,0
45,78,0
45,85,0
45,90,0
45,88,0
45,67,1
30,75,1
35,77,1
40,88,0
40,75,1
45,95,0
40,72,1
40,65,1
45,90,0
40,77,1
45,90,0
40,88,0
45,75,1
45,73,1
35,75,1
55,65,1
40,95,0
45,87,0
45,67,1
50,80,1
50,67,1
45,77,0
45,92,0
45,73,1
40,75,0
35,70,1
45,50,1
45,87,0
35,73,1
40,83,1
45,88,0
45,73,1
40,72,1
45,88,0
50,88,0
40,75,1
50,95,0
35,68,1
45,75,1
50,90,0
40,88,0
40,78,1
55,68,1
45,85,0
60,63,1
40,78,0
40,92,0
50,65,1
45,82,0
45,97,0
50,83,1
45,73,1
45,75,1
50,67,1
45,65,1
50,73,1
40,73,1
45,97,0
45,90,0
45,90,0
45,68,1
45,87,0
40,83,0
40,83,0
40,60,1
45,82,0
------------------------------------------------------------------------
Dataset:11.Logic Regression
length,width,type
1	,0.2,Iris-setosa
1.1	,0.1,Iris-setosa
1.2	,0.2,Iris-setosa
1.2	,0.2,Iris-setosa
1.3	,0.2,Iris-setosa
1.3	,0.4,Iris-setosa
1.3	,0.2,Iris-setosa
1.3	,0.2,Iris-setosa
1.3	,0.3,Iris-setosa
1.3	,0.3,Iris-setosa
1.3	,0.2,Iris-setosa
1.4	,0.2,Iris-setosa
1.4	,0.2,Iris-setosa
1.4	,0.2,Iris-setosa
1.4	,0.3,Iris-setosa
1.4	,0.2,Iris-setosa
1.4	,0.1,Iris-setosa
1.4	,0.3,Iris-setosa
1.4	,0.2,Iris-setosa
1.4	,0.2,Iris-setosa
1.4	,0.3,Iris-setosa
1.4	,0.2,Iris-setosa
1.4	,0.2,Iris-setosa
1.5	,0.2,Iris-setosa
1.5	,0.2,Iris-setosa
1.5	,0.1,Iris-setosa
1.5	,0.2,Iris-setosa
1.5	,0.4,Iris-setosa
1.5	,0.3,Iris-setosa
1.5	,0.4,Iris-setosa
1.5	,0.2,Iris-setosa
1.5	,0.4,Iris-setosa
1.5	,0.1,Iris-setosa
1.5	,0.1,Iris-setosa
1.5	,0.1,Iris-setosa
1.5	,0.2,Iris-setosa
1.5	,0.2,Iris-setosa
1.6	,0.2,Iris-setosa
1.6	,0.2,Iris-setosa
1.6	,0.4,Iris-setosa
1.6	,0.2,Iris-setosa
1.6	,0.2,Iris-setosa
1.6	,0.6,Iris-setosa
1.6	,0.2,Iris-setosa
1.7	,0.4,Iris-setosa
1.7	,0.3,Iris-setosa
1.7	,0.2,Iris-setosa
1.7	,0.5,Iris-setosa
1.9	,0.2,Iris-setosa
1.9	,0.4,Iris-setosa
3	,1.1,Iris-versicolor
3.6	,1.3,Iris-versicolor
3.8	,1.1,Iris-versicolor
3.9	,1.4,Iris-versicolor
3.9	,1.1,Iris-versicolor
3.9	,1.2,Iris-versicolor
4	,1.3,Iris-versicolor
4	,1.3,Iris-versicolor
4	,1.3,Iris-versicolor
4	,1.2,Iris-versicolor
4.1	,1.3,Iris-versicolor
4.1	,1.3,Iris-versicolor
4.2	,1.5,Iris-versicolor
4.2	,1.3,Iris-versicolor
4.2	,1.2,Iris-versicolor
4.2	,1.3,Iris-versicolor
4.3	,1.3,Iris-versicolor
4.3	,1.3,Iris-versicolor
4.4	,1.4,Iris-versicolor
4.4	,1.4,Iris-versicolor
4.4	,1.3,Iris-versicolor
4.4	,1.2,Iris-versicolor
4.5	,1.5,Iris-versicolor
4.5	,1.3,Iris-versicolor
4.5	,1.5,Iris-versicolor
4.5	,1.5,Iris-versicolor
4.5	,1.5,Iris-versicolor
4.5	,1.5,Iris-versicolor
4.5	,1.6,Iris-versicolor
4.5	,1.7,Iris-virginica
4.6	,1.5,Iris-versicolor
4.6	,1.3,Iris-versicolor
4.6	,1.4,Iris-versicolor
4.7	,1.4,Iris-versicolor
4.7	,1.6,Iris-versicolor
4.7	,1.4,Iris-versicolor
4.7	,1.2,Iris-versicolor
4.7	,1.5,Iris-versicolor
4.8	,1.8,Iris-versicolor
4.8	,1.4,Iris-versicolor
4.8	,1.8,Iris-virginica
4.8	,1.8,Iris-virginica
4.9	,1.5,Iris-versicolor
4.9	,1.5,Iris-versicolor
4.9	,1.8,Iris-virginica
4.9	,1.8,Iris-virginica
5	,1.7,Iris-versicolor
5	,1.5,Iris-virginica
5	,1.9,Iris-virginica
5.1	,1.6,Iris-versicolor
5.1	,1.9,Iris-virginica
5.1	,2.4,Iris-virginica
5.1	,1.5,Iris-virginica
5.1	,2.3,Iris-virginica
5.1	,1.9,Iris-virginica
5.1	,1.8,Iris-virginica
5.2	,2.3,Iris-virginica
5.3	,1.9,Iris-virginica
5.3	,2.3,Iris-virginica
5.4	,2.1,Iris-virginica
5.4	,2.3,Iris-virginica
5.5	,2.1,Iris-virginica
5.5	,1.8,Iris-virginica
5.5	,1.8,Iris-virginica
5.6	,1.8,Iris-virginica
5.6	,2.1,Iris-virginica
5.6	,1.4,Iris-virginica
5.6	,2.4,Iris-virginica
5.6	,2.4,Iris-virginica
5.7	,2.3,Iris-virginica
5.7	,2.1,Iris-virginica
5.7	,2.5,Iris-virginica
5.8	,1.8,Iris-virginica
5.8	,1.6,Iris-virginica
5.9	,2.1,Iris-virginica
5.9	,2.3,Iris-virginica
6	,2.5,Iris-virginica
6	,1.8,Iris-virginica
6.1	,2.5,Iris-virginica
6.1	,1.9,Iris-virginica
6.1	,2.3,Iris-virginica
6.3	,1.8,Iris-virginica
6.6	,2.1,Iris-virginica
6.7	,2.2,Iris-virginica
6.9	,2.3,Iris-virginica


